# Splunk
v. 2024-06-16

**Online Links**
- [Splunk how to video playlist](https://www.youtube.com/playlist?list=PL7zWAA-DF0k_sxswRiB7_GUTyI0FoV7lc)
- [Splunk Zero to Power User Udemy course](https://www.udemy.com/course/splunk-zero-to-power-user/?ranMID=39197&ranEAID=%2FjZHTpnCvx8&ranSiteID=_jZHTpnCvx8-pWhqxDYterEoOQCwKMql3g&LSNPUBID=%2FjZHTpnCvx8&utm_source=aff-campaign&utm_medium=udemyads&couponCode=KEEPLEARNING)
- Nice [rundown of regex](https://www.youtube.com/watch?v=QvxLJ9f6AK0) on Splunk How-To YouTube
- [Splunk Search Tutorial](https://docs.splunk.com/Documentation/Splunk/9.2.1/SearchTutorial/Usefieldstosearch)
- [Splunk Search Manual](https://docs.splunk.com/Documentation/Splunk/9.2.1/Search/GetstartedwithSearch)
- [Splunk Reporting Manual](https://docs.splunk.com/Documentation/Splunk/9.2.1/Report/Aboutreports)
- [regex101](https://regex101.com/) website is super useful
- [crontab guru](https://crontab.guru/) is super useful for cron schedule expressions
- [whois.domaintools.com](https://whois.domaintools.com/) is super useful

## Splunk Core Certified User

> [!Todo] Study Plan
> 
> - [x] Finish Search Tutorial
> - [x] Udemy: Zero to Power User
> - [x] STEP Splunk Training:
> 	- [x] Intro to Splunk
> 	- [x] Using fields  
> 	- [x] Scheduling reports and alerts  
> 	- [x] Visualizations  
> 	- [x] Working with Time  
> 	- [x] Statistical processing 
> 	- [x] Intro to Knowledge Objects
> - [x] Anki STEP Quiz Questions


## Splunk Docs mapped to Test Blue Print

**1.0 Splunk Basics**
- [ ] Splunk components - [Components of a Splunk enterprise deployment](https://docs.splunk.com/Documentation/Splunk/9.2.1/Capacity/ComponentsofaSplunkEnterprisedeployment)
- [ ] Understand uses of Splunk - [What is Splunk](https://www.splunk.com/en_us/blog/learn/what-splunk-does.html)
- [ ] Define Splunk apps - [Apps and add-ons](https://docs.splunk.com/Documentation/Splunk/9.2.1/Admin/Whatsanapp)
- [ ] Customizing user settings - [Update your user settings](https://docs.splunk.com/Documentation/SIM/current/User/Usersettings)
- [ ] Basic navigation in Splunk - [Navigating Splunk Web](https://docs.splunk.com/Documentation/SplunkCloud/latest/SearchTutorial/NavigatingSplunk)

**2.0 Basic Searching**
- [ ] Run basic searches - [Basic searches and search results](https://docs.splunk.com/Documentation/Splunk/9.2.1/SearchTutorial/Startsearching)
- [ ] Set the time range of a search - [Searching specific time ranges](https://docs.splunk.com/Documentation/SCS/current/Search/Specifyingtimeranges)
- [ ] Identify the contents of search results - [Basic searches and search results](https://docs.splunk.com/Documentation/Splunk/9.2.1/SearchTutorial/Startsearching)
- [ ] Refine searches - [Anatomy of a search](https://docs.splunk.com/Documentation/Splunk/latest/Search/Aboutsearchlanguagesyntax)
- [ ] Use the timeline - [Using the timeline](https://docs.splunk.com/Documentation/Splunk/latest/Search/Usethetimeline)
- [ ] Work with events
- [ ] Control a search job - [View search job properties](https://docs.splunk.com/Documentation/Splunk/9.2.1/Search/ViewsearchjobpropertieswiththeJobInspector),  [Manage search jobs](https://docs.splunk.com/Documentation/SplunkCloud/latest/Search/SupervisejobswiththeJobspage)
- [ ] Save search results - [Saving searches](https://docs.splunk.com/Documentation/Splunk/9.2.1/Search/Savingsearches)

**3.0 Using fields in searches**
- [ ] Understand fields - [Use fields to search](https://docs.splunk.com/Documentation/Splunk/9.2.1/SearchTutorial/Usefieldstosearch)
- [ ] Use fields in searches - 
- [ ] Use the fields sidebar - 

**4.0 Search Language Fundamentals**
- [ ] Review basic search commands general search practices - [Types of commands](https://docs.splunk.com/Documentation/Splunk/9.2.1/Search/Typesofcommands)
- [ ] Examine the search pipeline - [Anatomy of a search](https://docs.splunk.com/Documentation/Splunk/latest/Search/Aboutsearchlanguagesyntax)
- [ ] Specify indexes in searches - [Search Indexes](https://docs.splunk.com/Documentation/Splunk/latest/Search/Searchindexes)
- [ ] Use the following commands:
	- [ ] `tables` - [table](https://docs.splunk.com/Documentation/Splunk/latest/SearchReference/Table)
	- [ ] `rename` - [rename](https://docs.splunk.com/Documentation/Splunk/latest/SearchReference/Rename)
	- [ ] `fields` - [fields](https://docs.splunk.com/Documentation/Splunk/9.2.1/SearchReference/Fields)
	- [ ] `dedup` - [dedup](https://docs.splunk.com/Documentation/Splunk/latest/SearchReference/Dedup) 
	- [ ] `sort` - [sort](https://docs.splunk.com/Documentation/Splunk/latest/SearchReference/Sort)

**5.0 Using Basic Transforming Commands**
- [ ] `top` - [top](https://docs.splunk.com/Documentation/Splunk/latest/SearchReference/Top)
- [ ] `rare` - [rare](https://docs.splunk.com/Documentation/Splunk/latest/SearchReference/Rare)
- [ ] `stats`  - [stats](https://docs.splunk.com/Documentation/SplunkCloud/latest/SearchReference/Stats) 

**6.0 Creating Reports and Dashboards**
- [ ] Save a search as a report - [Save and share your reports](https://docs.splunk.com/Documentation/Splunk/9.2.1/SearchTutorial/Aboutsavingandsharingreports)
- [ ] Edit reports - [Create and edit reports](https://docs.splunk.com/Documentation/SplunkCloud/latest/Report/Createandeditreports)
- [ ] Create reports that display statistics (tables) - [Generate a table](https://docs.splunk.com/Documentation/Splunk/9.2.1/Viz/TableFormatsGenerate)
- [ ] Create reports that display visualizations (charts) - [Visualization reference](https://docs.splunk.com/Documentation/Splunk/9.2.1/Viz/Visualizationreference)
- [ ] Create a dashboard - [Create a dashboard](https://docs.splunk.com/Documentation/Splunk/9.2.1/Viz/CreateDashboards)
- [ ] Add a report to a dashboard - [Add panels to dashboard](https://docs.splunk.com/Documentation/Splunk/9.2.1/SearchTutorial/Addreportstodashboard)
- [ ] Edit a dashboard - [Edit dashboards](https://docs.splunk.com/Documentation/Splunk/9.2.1/Viz/DashboardEditor)

**7.0 Creating and Using Lookups**
- [ ] Describe Lookups
- [ ] Examine a lookup file example
- [ ] Create a lookup file and create a lookup definition
- [ ] Configure an automatic lookup
- [ ] Use the lookup in searches

**8.0 Creating Scheduled Reports and Alerts**
- [ ] Describe scheduled reports
- [ ] Configure scheduled reports
- [ ] Describe alerts
- [ ] Create alerts
- [ ] View fired alerts

## STEP Splunk Training

**Contents:**
- [[#Intro to Splunk]]
- [[#Using fields]]
- [[#Scheduling Reports & Alerts]]
- [[#Visualizations]]
- [[#Working With time]]
- [[#Statistical Processing]]
- [[#Intro to Knowledge Objects]]


### Intro to Splunk

Index - machine data from servers, network devices, etc.
- Data is originally ingested into the index

Data model = structured collections of datasets

3 predefined default Splunk Enterprise roles:
1. Administrator - can install apps, ingest data, create knowledge objects
2. Power - can create knowledge objects, share knowledge objects, and perform real time searches
3. User - can only see own knowledge objects

Additional Splunk Cloud roles:
1. sc_admin
2. power
3. user
4. can_delete
5. token_auth
6. apps

100s of apps available on Splunkbase

8 main components of Search app:
1. Splunk bar
2. App bar
3. Search bar
4. Time range picker
5. Table views (alternative way of querying data outside of searches)

Sourcetypes - data classifications
Hosts = origin I.P. address or FQDN



#### Using Search
- Limiting search by time vastly increases performance and is essential
- By default, search jobs remain active for 10 minutes
- Shared search jobs remain active for 7 days
- Search jobs can be exported as CSV, XML, JSON


**Transforming commands**: commands that create statistics and visualizations

3 search modes:
1. **Fast** - Field discovery is disabled. Only default fields
2. **Smart** - (Default) intelligently selects 
3. **Verbose** - Discovers all fields




#### Exploring Events
- Events are returned in reverse chronological order
- Timestamps are based on timezone set in user account
- "Selected fields" are displayed along bottom of event

3 options when rolling over text in an event:
1. `Add to search`
2. `Exclude from search`
3. `New search`

3 options for field actions:
1. `Build Event Type`
2. `Extract Fields`
3. `Show Source`

#### Using Search Terms

Wildcards: *

Searching for `fail*` will return any event that starts with `fail`:
1. `fail`
2. `failure`
3. `failed`

Order of operation for Booleans
1. NOT
2. OR
3. AND

Parentheses can be used to control order of evaluation

`NOT` will always return more results than `!=`

e.g. `failed NOT (success OR accepted)` <- Splunk computes parenthetical first

Searches are **not** case sensitive (although field names are)

Booleans `AND`, `OR`, and `NOT` can be used with multiple terms

- Exact terms are searched with quotes "failed"
- A backslash `\` escapes characters in a search


#### Commands

**5 components of queries:**
1. Search terms
2. Commands - what to do with search results (stats, charts, formatting)
3. Functions - how to chart, compute and evaluate results
4. Arguments - variables of functions
5. Clauses - how results are grouped and defined
 
![[Pasted image 20240429211813.png]]
![[Pasted image 20240616203921.png]]

`search` - can be used further down the query pipeline to filter results even more

If a command references a specific value, that value will be case sensitive (e.g. host names must be an exact case match)

Time is the most efficient way to filter events

Default fields:
1. Time
2. Index
3. Source
4. Host
5. Sourcetype

Inclusion ("accessed denied") is better than exclusion ("NOT access granted")

"user=admin OR user=administrator"

user IN (admin, administrator)

sid = Search ID



#### Knowledge objects 

**5 categories of knowledge objects:**
1. Data interpretation (e.g. fields, field extractions, calculated fields)
2. Data classification (Event Types, Transactions)
3. Data enrichment (Lookups, Workflow Actions)
4. Data normalization (Tags, Field Aliases)
5. Data models (Hierarchically structured datasets)

**Reasons KOs are useful:**
1. They can be shared
2. They can be saved and reused by people and apps
3. They can be used in search

**"Knowledge Managers" oversee KOs**
1. Oversee creation
2. Normalize event data
3. Implement naming conventions
4. Create data models

**Types of Knowledge Objects:**
- Fields - 
- Field extractions - 
- Calculated fields - added at search time based on values of existing fields
- Event types - categories of events based on search terms
- Transactions - groups of conceptually-related events that span time
- Lookups - allow you to add fields and values to your events
- Workflow actions - create interactive links within events that interact with external resources
- Tags - descriptive names for key-value pairs
- Field aliases - means of normalizing data over multiple sources
- Data models - hierarchically structured data sets

By default only Power and Admin users can share knowledge objects

#### Creating Reports
**Reports** - means of saving and sharing searches

`iplocation` command adds geographic context to event
`geostats` - command plots `iplocation` data on a map

Report naming convention:
1. Group
2. Type of object
3. Simple description

By default reports are only visible to creator

By default, displaying reports in "all apps" is only available to users with an admin role
Power users are granted read/write permissions on report

If you "run as" a report as a user rather than an owner, it will only show results based on what is available to that user

Scheduling reports may be more resource-efficient than running ad hoc searches

Scheduled reports are excellent for:
1. Shared report between many users
2. Report in an embedded dashboard

Trigger actions can output reports to an email, a webhook, or custom scripts


#### Creating dashboards

Any search that returns statistical values can be viewed as a chart

`top` command defaults to a limit of 20

2 types of dashboard creators:
1. Classic
2. Dashboard studio

Options in "Format" menu differ based on visualization type

"Drilldowns" let you add behaviors when a user interacts with a visualization (e.g. link to search, link to URL, etc.)


#### Dashboard studio
- You can clone classic dashboards in Dashboard Studio
- Dashboard Studio has 2 modes:
1. Absolute - much more customizable and precise
2. Grid - less so (also doesn't support images)

### Using fields

#### Using fields sidebar

Shows all fields extracted at search time

2 sections:
1. Selected fields - important fields show up under events and at top of sidebar
2. Interesting fields - occur in at least 20% of events

`a` in fields sidebar indicates a string
`#` in fields sidebar indicates a numerical value

#### Fields in search queries

`sourcetype=linux_secure` returns all events whose sourcetype is `linux_secure`

Field names (`sourcetype`) are case sensitive
Field values (`linux_secure`) are not case sensitive

**Field operators:**
- `=` and `!=` can be used with numerical or string values
- `>, >=, <, <=` can only be used with numerical values

Fields can also be added to a search through the field window

For field values with IP addresses wildcards (`*`) are subnet-aware

`!=` vs. `NOT`:
- `!=` - only returns events that contain the field but not the specific value
- `NOT` - returns all events regardless of whether or not they contain the field in the search

`IN` operator 

`| fields` command can include or exclude fields in a search

Including `| fields` early on in a search makes it more efficient by eliminating extraneous field searches. For example

```
index=web status IN ("500", "503", "505")
| fields status
| stats count by status
```


`+` or `-` operator with `fields` includes or excludes fields (e.g. `fields -status`)


`| rename` - renames fields in a search for usability

```
| rename status as "HTTP Status"
```




#### Fields in search results

 **Temporary fields** - created ad hoc with commands like `| eval`

`| eval` command calculates and manipulates field values

Fields can be extracted with:
1. `| erex` - give sample of values, not persistent
	1. Using `where isnull` is a good way to see if any values have been missed
2. `| rex` - can match multiple groups=multiple fields, not persistent 
	1. Running `| rex field=_raw` can cause performance issues
3. Field extractor - persistent across searches


#### Enriching data with knowledge objects

Field aliases can be applied across sources with shared aliases

Lookups - can add fields and values not in original data

Search time order of operations:
1. Field extractions
2. Field aliases
3. Calculated fields
4. Lookups
5. Event Types
6. Tags

So, field extractions cannot reference tags.


### Scheduling Reports & Alerts
#### Creating Scheduled Reports

Steps to create a scheduled report:
- Create and run a search
- Click "Save As" report
- After saving, click "Schedule" under additional settings
- Toggle "Schedule Report" and configure intervals and time range

**Schedule Window** setting - can set looser timeframe in which to run report (allowing for flexibility if system is busy running other reports)

Possible scheduled report actions:
1. Log event to Splunk receiver
2. Output results to CSV lookup
3. Output results to telemetry endpoint
4. Run custom script
5. Send email
6. Webhook


#### Managing Reports

Use Settings > "Searches, reports, and alerts" link or "Reports" tab

From this screen you can edit:
- Permissions
- Schedule
- Acceleration
- Summary indexing

Users with Power role can edit permissions to make a report visible for all users in an app

Users with Admin role can edit report permissions to make it visible for all apps

Embedded report produces iframe that is viewable by anyone with access to webpage

#### What are Alerts?

Splunk alerts - based on defined conditions in completed searches (whether scheduled or ad hoc)

Alerts can be configured to:
- List in interface
- Log events
- Output to lookup
- Send to a telemetry endpoint
- Trigger scripts
- Send emails
- Use a webhook
- Run a custom alert


#### Creating Alerts

1. Define a search
2. Click "Save As > Alert"

Alerts can be either:
1. Scheduled
2. Real-time (run continuously (so high overhead))

Alert triggers can be set:
1. Per-result
2. After a number of results (e.g. # results with an error > 5)
3. After a number of hosts
4. Number of sources
5. Or custom

Triggers can be throttled to fire at different frequencies


#### Using Alert Actions

- Alert actions respond to triggered alerts
- Possible alert actions:
	- "Triggered Alert" action - designates severity for alert
	- "Log event" action - sent to Splunk deployment for indexing
	- "Output results to lookup" - creates or updates CSV
	- "Output to telemetry endpoint"
	- "Run a script" <- this is officially deprecated
	- "Send email"
	- "Webhooks"


#### Managing Alerts

"Activity" > "Triggered alerts" in main Splunk app menu can show all triggered alerts

"Alerts" tab in "Search & Reporting" app also lists all triggered alerts

Alerts are private by default


### Visualizations

#### Using Formatting Commands
`| fields` can include/exclude particular fields (optimizing search time)
	`| fields - <fieldname> <fieldname>` excludes both fields
	Omitting space after dash (`-`) attaches it to just that particular field name

`| table` transforming command that displays data in a tabulated format

`|dedup` removes duplicate results from search that display duplicate values (can be used for single field or multiple fields)

`|addtotals` computes sum of all numeric fields for each row
	`| addtotals col=true label="Total Sales"` adds a row to the bottom

`|fieldformat` change data formatting without changing underlying raw data in index



#### Visualizing Data
Any search that returns statistical values can be returned in a chart

Most visualizations require results returned in tables of at least two columns

List of Splunk transforming commands:
- `top` - finds most common values of given fields. Common clauses:
	- `limit = int` - limits results number
	- `countfield = string` - labels count
	- `percentfield = string` - labels percent
	- `showcount = True/False`
	- `showperc = True/False`
	- `by` clause 
- `rare` - finds least common values of fields. Has same clauses as `top`.
- `stats` - produces statistics from search results
	- `count` - use `as` clause to rename title
		- `stats count(<fieldname>)` - gives count of number of events in a field
	- `distinct count`
	- `sum`
	- `average`
	- `min`
	- `max`
	- `list`
	- `values`
- `chart` - take two clause statements
	- `over` - field for x axis (e.g. `chart over status` shows status field on x axis)
	- `by` - only one field can be specific after over
	- `usenull` and `useother` default to True in chart command
	- `limit` argument limits number of plotted series 
- `timechart` - stats aggregations against time (time is always the x axis)
	- `limit`, `usenull` and `useother` options all available
	- `span` argument specifies time range
- `trendline` - computes moving averages of field values
	- requires 3 arguments:
		- trendtype:
			- `SMA`
			- `EMA`
			- `WMA`
		- Time period
		- Field
	- `| trendline wma2(sales`) computes WMA for sales over the last 2 days

#### Generating Maps

2 types of maps
1. **Marker map**
2. **Choropleth** - shading for relative metrics for regions
`| iplocation` - correlates event IP with location from third-party database

Can include: City, County, Region, Latitude, and Longitude

`|geostats` - aggregates data on a map with required latitude and longitude values
- Only accepts one `by` argument

`iplocation` produces `lat` and `lon` values. These can be piped to `geostats`, e.g. `|geostats latfield-lat longfiel=lon count`

`.kmz` file - Keyhole Markup Language file

Splunk ships with a U.S. states `.kmz` and a world countries `.kmz` by default

`|geom` - adds fields to event that matches polygon on map


#### Single Value Visualizations

2 types of Single Value Visualization
1. Single value - shows single formatted integer
2. Gauges: Formattable radial, pillar, and marker gauges
#### Formatting Visualizations

Visual formatting options:
- Formatting option in statistics tab: wrapping, cell or rows, data overlays, totals, percentages
- Chart overlay


### Working With time

#### Searching with Time

`_time` field is stored in index prior to search time (along with host, source, and sourcetype)

- Timestamps are in epoch or Unix time and translated to human readable format at search time (so timestamps are directly derived from `_time` field).

**Timeline** - shows distribution of events in the time range

Click-dragging on portion of timeline selections subsection of timeline and updates results (however, n.b. that search itself will not rerun)

Advanced section of time range picker allows you to pick time abbreviations

... `earliest=[+|-]<timeInt><timeUnit>@<timeUnit>`
... `latest=[+|-]<timeInt><timeUnit>@<timeUnit>`

Including these in search overrides time range picker

`@<timeUnit` snaps, and always rounds down (going backward in time)

`@w0` = Sunday
`@w1` = Monday

Snap to `@h` sets everything to the right of the hour digit to 0


Assuming time is 09:45:00 on April 1st 2021:
- `30m@h` -  looks back to 09:00:00 on Apr 1st 2021
- `earliest=-h@h` - looks back to 8:00:00 on Apr 1st 2021
- `earliest=-mon@mon latest =@mon` - Looks for events from 00:00:00 on March 1 2021 to 00:00:00 on April 1 2021
- `earliest =-7d@d` - Looks for events from 00:00:00 on March 25 2021 (7 days before April 1) to 09:45:00 on Apr 1st 2021
- `earliest=@d+3h` - Looks for events from 0:300:00 to 09:45:00 on April 1 2021

Every event with a timestamp has `date_*` fields computed automatically:
- `date_hour`
- `date_mday`
- `date_minute`
- `date_month`
- `date_second`
- `date_wday`
- `date_year`
- `date_zone`

Not all events have a timestamp in the raw event, if Splunk cannot find a `_time`, it falls back to the index time.

`_time` is set to GMT

Differential between `_time` and generated timestamp is if User Settings specify a different time zone than GMT

`bin` - puts continuous numerical values into discrete sets. So `| bin span=1h _time`, rounds all time values into the nearest hour value
`strftime(<fieldwithtimevalues>, "%b %d, %I %p")`

`bin` timescale values include sec, min, hr, day, month, subsecond



#### Formatting Time

`| eval` - writes calculated expression into a new or existing field with tons of functions


Time and data functions of `eval`:
- `now()` - returns the time a search was started
	- `| eva field1 = now()`
- `time()` - returns the time an event was processed by `eval` command
	- `| eval field1 = time()`
- `relative_time()` - returns an epoch timestamp relative to a supplied time
	- `| eval field1 = relative_time(<DesiredEpochTime,RelativeTimeSpecifier`
	- `| eval yesterday = relative_time(now(),"-1d@h")`



`strftime` - formats epoch time values into a specific format with date/time variables
- `eval formatted_time = strftime (_time, "%b %d, %I %p")` reformats 2"021-05-13 19:00" as "May 13, 07 PM"

`strptime`- formats a string with a time into a UNIX timestamp based on specific formatting
- `| eval NewAsctime = strptime(asctime, "%Y-%m-%d %H:%M:%S,%N")` reformats "2020-03-05 21:42:55,814" to "1583444575.814000"



#### Using Time Commands

`timechart` - transforming command that performs statistical aggregations against time
- Commonly used with `count` and `sum` functions
- Defaults to top 10 values like `| chart`, but has `limit` option

`| timewrap` - display output of `timechart` so that each time period is separate series. Typically follows `timechart` command in SPL
- Compares data over specific time period, such as day-over-day or month-over-month
- `| timewrap [<int>]<timescale>`

`index=security sourcetype=linux_secure "failed password" earliest=-12@d latest=@d` - searches for failed password attempts over last two weeks.


The below snippet compares failed password attempts from week to week:

```
index=security sourcetype=linux_secure "failed password" earliest=-12@d latest=@d
| timechart span=1d count as Failures
| timewrap 1w

```

adding `| eval Day = strftime(Day, %A)` reformats time to just show the weekday


#### Working with Time Zones

To create a secondary time column with UTC time if user settings have local time, use `| eval Hour=strftime(_time, "%H")`


### Statistical Processing

#### What is a Data Series?

**Data series**: Sequence of related data points plotted on a visualization
- Single series - compares points of single data category
- Multi-series - points of two or more categories
- Time series - related data points over time (single or multi-series)

We'll compare these commands:
- `chart`
- `timechart`
- `top`
- `rare`
- `stats`

#### Chart Command

`chart` takes results and returns them formatted in a table
`over` clause specifies x-axis
using `over` and `by` adds an additional split to the chart
`span` argument groups data points into range buckets (i.e. 403 would fall into 400-500 bucket)

#### Timechart Command

Stat aggregations against time

`_time` is always the x-axis

Can only split by one field with `timechart`

Buckets are automatically assigned based on time range: (e.g. 60 minute search would create `span=1m`) This can be overridden with `span` argument.

`limit` option control distinct values returned with `by` clause


#### Top Command
- Finds most common values for a field 
- Arguments:
	- `showperc=t` and `showperc=f` control is percentages are shown
	- `countfield` - renames "count" column label to whatever you choose
- `top` can also be run from the field details popup window
- `limit` argument for `top` is 20 by default
#### Rare Command
- Same options as `top` command

#### Stats Command
- `stats` uses functions to calculate statistics on search results, produce table
- `stats` allows for continuous splits of data
- `as` clause renames the count field

#### Functions of the Stats Command

4 categories of statistical functions:
1. **Aggregate** - summarize event values to create single value
2. **Event Order** - return values from fields based on processing order
3. **Multivalue** - return lists of values for a field as a multivalue entry
4. **Time** - returns values based on time

- `dc` - count of unique values for a given field
- `sum`
- `min`, `max`, `avg` for given fields
- `list` versus `values` - `values` will only show unique values for a given field with another field. List includes all duplications.
#### Transforming Commands Summary
- `chart/timechart` end result: chart
- `stats` end result: table

#### Eval Command
- Performs calculations with values in data
	- Writes result to a new field or overwrites existing field
- Can use `eval` command multiple times on same field
- `round` function takes the field you wish to round and the decimal place you wish to round to

#### Functions of the Eval Command
- Some of the most common:
	- `pow()` - returns x raised to the power of y
	- `round()` - commonly wraps another mathematical process to make it human-readable
	- `max()`
	- `min()`
	- `random()` - returns pseudo-random integer


#### Eval as a Function

- `eval` can be used as a function of the `stats` command
- `| stats count(eval(vendor_action="Accepted")) as Accepted,`
#### Rename Command

- `rename <field> as <newfield>`
- Use double quotes if fieldname contains special characters
- New name of field must be used for remainder of search
- Can use wildcards to rename multiple fields with the same prefixes


#### Sort Command
- Sorts in ascending order by default
- `| sort (-|+) <field>`
- Uppercase letters appear before lower case (A, B, C, a, b, c instead of A, a, B, b, C, c)

### Intro to Knowledge Objects
- **Some examples of KOs:**
	- Fields
	- Field aliases
	- Field extractions
	- Calculated fields
	- Lookups
	- Tags
	- Workflow actions
	- Reports & Alerts
	- Macros
	- Data models

#### Knowledge object settings
- Naming conventions with six segment key:
	- Group
	- Type
	- Platform
	- Category
	- Time
	- Description
- e.g. `OPS_WFA_Network_Security_na_IPwhoisAction`

3 predefined permissions for KOs:
1. Private
2. Specific App
3. All apps

Admin is only role able to grant "All Apps" permission







---

Selected fields - fields of importance to user. Listed in sidebar and at bottom of every event.

3 default or "selected fields" for every event:
1. Host
2. Source
3. Sourcetype

Interesting fields appear in at least 20% of events

Other default fields that are created when data is indexed:
1. `timestamp` fields begin with `date_*`



letter `a` indicates the field is a string
hashmark `#` indicates the field is numerical

field names are case sensitive

field operators `=` and `!=` can be used with numerical or strings

So, `host!="mail*"` is valid search syntax

`>`, `>=`, `<`, `<=` field operators can only be used with numerical

Wildcards can be used with IP addresses to identify hosts on subnets

`!=` and the boolean `NOT` may return different results, because `NOT` includes events that do not contain the field type in the first place.

`status IN ("500", "503", "505")` returns the same results as `status=500 OR status=503 OR status=505`

`| fields` command can include or exclude field types
`| fields -status` will exclude the status field
`| fields +status` will include the status field

`| rename` command renames fields to something more practical
`| rename status as "HTTP Status", count as "Number of Events"` will rename both fields

Field extractor utility, or commands `| erex` and `| rex` can use regex to extract fields not detected automatically


`| erex` automatically populates a field from a few examples, e.g. `| erex Character fromfield=_raw examples=pixie, Kooby`

"Job" button can show what regex expression was used by `erex`

`| rex` allows you to match multiple groups

`erex` is easier to use, because `rex` requires use of regex

`erex` requires sample data, whereas `rex` does not

`erex` generates Regex, but `rex` is faster

Order in which field operations occur:
1. Field extractions
2. Field aliases
3. Calculated fields
4. Lookups
5. Event types
6. Tags

Many transforming commands return additional new fields (e.g. `top` will produce `count` and `percent` fields)

Subsearches are enclosed in square brackets and are evaluated first

By default, subsearches return a maximum of 10,000 results and have a maximum runtime of 60 seconds.

`ctrl+\` auto-formats searches to make them easier to parse

`AS` command renames fields to be more readable






---
## Zero to Power User



> [!Note] Note
> Udemy Course finished May 2024

Important terms:
- SIEM - Security Information and Event Management
- SPL - Search Processing Language

Splunk: Network analysis tool for big data analytics

### Core components
1. **F: Forwarder** - feeds raw data to the index.
	1. Universal forwarder
	2. Heavy forwarder
	3. Intermediary forwarder
3. **I: Indexer** - processes data, stores events
	1. Buckets - stored directory of time-grouped data that lives on indexer
4. **SH: Search Head** - executes search requests, SPL interface
	1. Searching by time is the most efficient delimiter because it leverages buckets 

Types of Splunk deployments:
- Standalone: single AIO deployment, local installation
	- No forwarders, only SH + I
- Basic: uses forwarders on remote machines that feed data to Splunk instance with SH + I
- Multi-instance: functionally separates SH, Indexers, and Forwarders and can have multiple machines in each role
- Clustering: uses multiple cloned SHs (minimum of 3) to scale querying
	- SH clusters are managed by deployer D
	- Can also replicate Indexers for redundancy and speed

### Getting Data into Splunk

4 phases of data pipeline:
1. Input (data = streams)
2. Parsing (data = events)
3. License Usage (license meter check)
4. Indexing (data = compressed)

Input types:
1. Files and directories
2. Network traffic
3. Log files
4. HEC (HTTP Event Collector)

Metadata:
1. Source = path or method of data (e.g. `values(source)` could be `access.log`, `cisco_ironport_web.log`, `secure.log`)
2. Host = who sent the data (e.g. `values(host)` could be `web1`, `web2`, `web3`)
3. Source Type = data format (e.g. `values(sourcetype)` could be `access_combined`, `cisco:wsa:squid`, `linux_secure`)

TA = Technology Add-ons

Apps are generally separate GUIs from Splunk (e.g. AWS, Azure, Corelight, Enterprise Security).

Apps fall on the search head.

Addons don't generally change workstation design or have separate GUI (e.g. CrowdStrike, Palo Alto, Linux)

Addons can fall on the indexer, forward, or search head components

### Basics of Searching

Searching by a field (i.e. `method=GET` instead of raw string `GET` is less resource-intensive because Splunk knows which field to search)

Boolean operators:
- OR (e.g. `index=web OR index=security` gives results in both)

Comparison operators:
- `bytes<268`, `bytes>268`, `bytes!=268` are all valid inputs

`!=` is not the same as `NOT`:
`NOT` always returns a greater number of events
`!=` always returns a lesser number of events

### Knowledge Objects

Useful tools or frameworks

Examples of KOs:
1. Alert for when 50 sales on website have occurred
2. Tag on events to show in green when user logs into website

Other examples:
- Fields
- Field extractions
- Lookups
- Tags
- Field alias
- Data model
- Saved Searches

Knowledge Managers - oversees KO creation, permissions, and naming conventions

Typical KO naming convention:

`<Group name>_<type>_<description>`
example: SOC_Alert_LoginFailures

3 permission levels for KOs:
1. Private
2. This app only
3. All apps

### Fields

Key value pairs
Field names **are** case sensitive
Field values are **not** case sensitive

### SPL

Basic SPL commands:

`table`
`rename`
`fields`
`dedup`
`sort`

Color coding for Splunk syntax:

Orange = command modifiers (e.g. `OR`, `NOT`, `AND`, `as`, `by`)
Blue = commands (e.g. `Stats`, `Table`, `Rename`, `Dedup`, `Sort`, `Timechart`)
Green = arguments (e.g. `Limit`, `Span`)
Purple = functions (e.g. `Tostring`, `Sum`, `Values`, `Min`, `Max`, `Avg`)

`Dedup` = ("reduplicate") removes duplicate values

`Ctrl+Shift+E` expands search editor, which can be helpful when employing macros

This search will show all unique matches for the clientip field:

```
index=acccess
| table clientip
| dedup clientip
```

The same results can be accomplished with this search:

```
index=access
| stats count by clientip
```

### Transform commands

Search command that orders commands in data table

Results of a transforming command will result in numerical values in a table

`Top` - finds top common values of a field. Limit=10 by default.

`Rare` - finds least common values of a field

`Stats` - calculate statistics
- `count`
- `dc`
- `sum`
- `avg`
- `list`
- `values`

### Events

`transaction` command

Some `transaction` arguments:
1. `maxspan` - max time between all related events (sets span of transaction)
2. `maxpause` - max time between each individual event (default = 1 min)
3. `startswith` and `endswith` - sets variables (e.g. Windows EventIDs)

Use cases:
1. Events that span time (even from multiple hosts)
2. Grouping of events (showing start and end)
3. Relates user activities, e.g. logins, session lengths, browsing history
4. Log validation (correlate network logs and data)

`transaction` is slower, more granular and more resource-intensive `stats`

### Manipulating Data

`eval` command:
- Calculates fields
- Arguments include `if`, `null`, `true`, `cidrmatch`, `like`, `lookup`, `tostring`, `md5`, `now`, `strftime` 
- Create new fields from the result of the SPL search
- Does not re-index any data; only manipulates pre-existing fields

2 basic `eval` use cases:

1. Convert epoch time to human-readable time
2. Running if statements

`where` command:
- Can't place before first pipe (`|`)
- Comparing values or searching for matching value

`search` command:
- Goes anywhere in SPL
- Search by keyword or matching value

This search converts internal epoch time to a human-readable format:
```
index="_internal" 
| eval epoch_time = strptime(_time, "%s")
| eval human_readable_time = strftime(epoch_time, "%m/%d/%y")
| table _time, epoch_time, human_readable_time
```

This search provides a count of the provided status HTTP codes:

```
index=access
| eval status_codes = case((status == 404), "not found", (status = 200), "success")
| stats count by status, status_codes
```

This search provides a table of files with their associated hash values:

```
index=access 
| eval hash=md5(file)
| table file, hash
```

### More Fields

Field extraction methods:
1. Regex (unstructured data)
2. Delimiters (structured)
3. Commands (`rex` and `erex`)

3 ways to get to field extractors:
1. `Settings Menu > Fields > Field extractions > Open Field Extractor`
2. In fields bar menu on search app
3. Event actions drop down menu

2 field extraction commands:
1. `erex` command
2. `rex` command

`rex` command accepts raw regex code to perform searches:

```
index=cisco
| rex field=_raw (?<email>"\S+@\S+.com")
```

`erex` command intelligently accepts example strings and creates its own regex:

```
index=cisco
| erex files examples="text/css", "image/gif"
```





### Lookups
Lookup - file that contains static data not in an index

KV sources allow use of dynamic data

Lookups add additional fields to search for

`inputlookup` search contents of lookup table. (e.g. `| inputlookup peopleinfo.csv where (first_name=Henry)

`outputlookup` outputs lookup table

Creating Lookups:
1. `Settings` > `Lookups` > `Lookup table files` > `New Lookup Table File`


Visualization charts:
1. Tables - single or multi-series (must use `chart` or `timechart`to create multi-series)
2. Charts 
3. Maps

difference between `chart`, `timechart`, and `stat`:
- `timechart` - time series showing statistical trends over time
- `chart` - Line, area, bar, bubble, pie, scatter, etc. Stacking available with trendline layers
- `stats` - can easily alter any stats in table

Panel options:
1. **Stacking** - on: events stacked vertically. Off: horizontally displayed
2. **Overlay** - add line charts over each other
3. **Trellis** - displays multiple charts, one for each category
4. **Multi-series**: if fields are sharing y-axis

4 commands to improve panel displays:
1. `iplocation` - add location info to visualizations
2. `geostats` - calculate functions to display on a cluster map
	1. Takes `latfield`, `longfield`, `globallimit`, and `locallimit`
3. `addtotals` - add multiple values together on a chart, Can achieve same result by editing formatting
	1. Takes `fieldname`, `label`, `labelfield`
4. `trendline` - Overlay on a chart to show the moving average
	1. Takes `sma`, `ema`, `wma`
	2. [Good article on the differences between SMA, EMA, and WMA](https://www.investopedia.com/ask/answers/071414/whats-difference-between-moving-average-and-weighted-moving-average.asp)

`sma` must include a digit specifying the length to average over, like this: `sma5`


### Drilldowns and Reports

Report - saved search
- Live results - re-run a report, or schedule it
- Reports are sharable knowledge knowledge object

Drilldowns add functionality to visualizations on dashboards
- Actions
	- Link to search
	- Link to dashboard
	- Link to report
- Tokens pass variables from panel to panel
- Export as PDF, print, or include in report

---

Tags are case sensitive


3 main workflow actions:
1. GET - create HTML links to interact with sites
2. POST - create entries in management systems
3. Search - secondary search based on selected fields within an event

Datamodel - hierarchical arrangement of datasets

`datamodel` command will pull data from a designated model

`tstats` searches indexes in accelerated datamodel

### CIM: Common Information Model

CIM - common standard for data operations in add-on form. "Data normalizer."
